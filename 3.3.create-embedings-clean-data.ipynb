{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687f7014-595b-488d-9a70-63f6fc6fff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install nltk\n",
    "#!pip install google-cloud-aiplatform>=1.38.0\n",
    "#!pip install pgvector\n",
    "#!pip install psycopg2-binary\n",
    "#!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af3945-3450-45fc-bdaf-39426f642c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=key.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05bca3-3aa0-4790-8f5c-badc0bede641",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GOOGLE_PROJECT_NUMBER="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf293d2b-0793-4c18-a960-4d10c3ce827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PG_VECTOR_PASSWORD="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a705fc76-f095-4de0-bb80-22c969d31ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.vectorstores import PGVector\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores.pgvector import DistanceStrategy as ds\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97816f-b89e-403a-bab3-8ae4b3cb3d47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download nltk tokenizers\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e91336-c575-4db3-bba7-d98d2dc43467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_from_documents(directory):\n",
    "    \"\"\"\n",
    "    create_chunks_from_documents Function Documentation\n",
    "\n",
    "    Description:\n",
    "        This function takes a directory path containing documents as input and creates text chunks from the documents using the NLTKTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The path to the directory containing documents.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks extracted from the documents in the specified directory.\n",
    "\n",
    "    Dependencies:\n",
    "        - DirectoryLoader: A class responsible for loading documents from a given directory.\n",
    "        - NLTKTextSplitter: A class that utilizes NLTK (Natural Language Toolkit) for splitting text into chunks.\n",
    "\n",
    "    Usage:\n",
    "        1. Provide the path to the directory containing documents as the 'directory' parameter.\n",
    "        2. The function loads the documents using the DirectoryLoader.\n",
    "        3. It then uses the NLTKTextSplitter to split the loaded documents into text chunks.\n",
    "        4. The resulting list of text chunks is returned.\n",
    "\n",
    "    Example:\n",
    "        directory_path = '/path/to/documents'\n",
    "        chunks = create_chunks_from_documents(directory_path)\n",
    "        print(chunks)\n",
    "\n",
    "    Note:\n",
    "        - Make sure to have the necessary dependencies installed before using this function.\n",
    "        - Adjust the 'chunk_size' and 'chunk_overlap' parameters of NLTKTextSplitter for customized chunking behavior.\n",
    "    \"\"\"\n",
    "    doc_loader = DirectoryLoader(directory)\n",
    "    documents = doc_loader.load()\n",
    "\n",
    "    text_splitter = NLTKTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b48418b-e1cd-4e4d-a1a6-26f644e4fd0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use project number instead of project name\n",
    "# https://stackoverflow.com/questions/66518534/httperror-403-with-consumer-invalid-as-the-reason-when-deploying-a-machine-learn\n",
    "project_number = os.environ[\"GOOGLE_PROJECT_NUMBER\"]\n",
    "#embed_model = VertexAIEmbeddings(project=project_number,model_name=\"textembedding-gecko@003\")\n",
    "embed_model = VertexAIEmbeddings(project=project_number,model_name=\"textembedding-gecko-multilingual@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c309c45-b3e4-43b2-9991-f791d966a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_chunks = create_chunks_from_documents(\"./data-clean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3603b6-82e7-40ee-b11d-1528d8a1e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection string for Postgres database\n",
    "password = os.environ[\"PG_VECTOR_PASSWORD\"]\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://mario:%s@postgres_pgvector:5432/test_db\" % quote(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcaf4f0-289f-483c-9566-85f2955de404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables with embeddings and metadata\n",
    "# langchain_pg_collection contains collection names and metadata\n",
    "# langchain_pg_embedding contains embeddings with documents and metadata\n",
    "PGVector.from_documents(documents=all_docs_chunks, embedding=embed_model, collection_name=\"all_docs_multi\", connection_string=CONNECTION_STRING)\n",
    "PGVector.from_documents(documents=all_docs_chunks, embedding=embed_model, collection_name=\"all_docs_multi_l2\", connection_string=CONNECTION_STRING, distance_strategy=ds.EUCLIDEAN)\n",
    "PGVector.from_documents(documents=all_docs_chunks, embedding=embed_model, collection_name=\"all_docs_multi_inner\", connection_string=CONNECTION_STRING, distance_strategy=ds.MAX_INNER_PRODUCT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
